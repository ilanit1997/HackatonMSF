{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### general functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_between_brackets(text):\n",
    "    pattern = r'\\[(.*?)\\]'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "\n",
    "def remove_text_between_brackets(text):\n",
    "    pattern = r'\\[.*?\\]'\n",
    "    result = re.sub(pattern, '', text)\n",
    "    return result\n",
    "\n",
    "\n",
    "def num_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def convert_utc_to_datetime(utc_time):\n",
    "    return datetime.datetime.utcfromtimestamp(utc_time).strftime('%d/%m/%y %H:%M:%S')\n",
    "\n",
    "\n",
    "def remove_text_after_x(text, x):\n",
    "    pattern = re.escape(x) + r'.*'\n",
    "    result = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_data_from_comment(comment, prefix=None):\n",
    "    data = dict(comment_id=comment.id, comment_body=remove_text_after_x(comment.body, 'edit:'),\n",
    "                comment_author=comment.author,\n",
    "                comment_date_time=convert_utc_to_datetime(comment.created), comment_score=comment.score)\n",
    "    if prefix is not None:\n",
    "        data = {f\"{prefix}_{k}\": v for k, v in data.items()}\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_data_from_post(post):\n",
    "    post_data = dict(post_id=post.id, title=remove_text_between_brackets(post.title),\n",
    "                     post_author=post.author,\n",
    "                     body=remove_text_after_x(post.selftext, 'edit:'), date_time=convert_utc_to_datetime(post.created),\n",
    "                     url=post.url, score=post.score, num_comments=post.num_comments)\n",
    "    post_data['flairs'] = str(extract_between_brackets(post.title))\n",
    "    comments = [(comment, comment.score, num_words(comment.body)) for comment in post.comments]\n",
    "    comments = [x for x in comments if x[2] > 30]\n",
    "    highest_comment = sorted(comments, key=lambda c: c[1], reverse=True)[0][0]\n",
    "    longest_comment = sorted(comments, key=lambda c: c[2], reverse=True)[0][0]\n",
    "    post_data.update(extract_data_from_comment(highest_comment, 'highest'))\n",
    "    post_data.update(extract_data_from_comment(longest_comment, 'longest'))\n",
    "    return post_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### reddit api"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'yy'\n",
    "CLIENT_SECRET = 'xx'\n",
    "USER_AGENT = 'my_user_agent'\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=USER_AGENT)\n",
    "subrredit_str_lists = ['abusiverelationships',  'ToxicRelationships', 'domesticviolence', 'abusesurvivors', 'emotionalabuse', 'Infedelity', 'survivinginfidelity', 'relationship_advice', 'relationships',\n",
    "'LifeAfterNarcissism', 'NarcAbuseAndDivorce', 'NarcissisticSpouses']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get all the data from the previous runs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "exisiting_data = []\n",
    "for i in range(1, 3):\n",
    "    root = f\"/data/home/ilanit.sobol/dv/data/outputs/reddit/subreddits/new/iter{i}\"\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".csv\"):\n",
    "            exisiting_data.append(pd.read_csv(f\"{root}/{file}\"))\n",
    "queries_root = \"/data/home/ilanit.sobol/dv/data/outputs/reddit/subreddits/queries\"\n",
    "\n",
    "for subreddit in os.listdir(queries_root):\n",
    "    for sort in os.listdir(f\"{queries_root}/{subreddit}\"):\n",
    "        for file in os.listdir(f\"{queries_root}/{subreddit}/{sort}\"):\n",
    "            if file.endswith(\".csv\"):\n",
    "                try:\n",
    "                    exisiting_data.append(pd.read_csv(f\"{queries_root}/{subreddit}/{sort}/{file}\"))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "exisiting_data = pd.concat(exisiting_data)\n",
    "exisiting_data = exisiting_data.drop_duplicates(subset=[\"post_id\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scrape the top posts from the subreddits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for subrredit_str in subrredit_str_lists:\n",
    "    current_data = []\n",
    "    subreddit_obj = reddit.subreddit(subrredit_str)\n",
    "    posts = list(subreddit_obj.top(time_filter=\"all\", limit=1000))\n",
    "    for post in tqdm(posts):\n",
    "        if post.id in exisiting_data.post_id.values:\n",
    "            continue\n",
    "        try:\n",
    "            post_data = extract_data_from_post(post)\n",
    "            post_data.update(dict(subreddit=subrredit_str))\n",
    "            current_data.append(post_data)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    df = pd.DataFrame(current_data)\n",
    "    df.to_csv(f'/data/home/ilanit.sobol/dv/data/outputs/reddit/top/{subrredit_str}.csv', index=False)\n",
    "    all_data.extend(current_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## search via query and sort"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "queries = [\"marriage\", \"relationship\", \"love\", \"hate\", \"like\", \"partner\", \"ex\", \"divorce\", \"work\", \"done\", \"everything\", \"right\", \"wrong\", \"simple\", \"know\", \"boyfriend\", \"flowers\", \"date\", \"clothes\", \"story\", \"together\", \"breakup\", \"social\", \"mental\", \"health\", \"hard\", \"we\", \"me\", \"living\", \"M39\", \"M20\", \"M21\", \"M42\",  \"M43\", \"M54\"]\n",
    "sorts = [\"hot\", \"new\", \"relevance\"]\n",
    "subrredit_str_lists = ['abusiverelationships',  'ToxicRelationships', 'domesticviolence', 'abusesurvivors', 'emotionalabuse', 'Infedelity', 'survivinginfidelity', 'relationship_advice', 'relationships', 'LifeAfterNarcissism', 'NarcAbuseAndDivorce', 'NarcissisticSpouses']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for sort in sorts:\n",
    "    for query in queries:\n",
    "        for subrredit_str in subrredit_str_lists:\n",
    "            folder = f'/data/home/ilanit.sobol/dv/data/outputs/reddit/queries/{query}/{sort}'\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "            if os.path.exists(f'{folder}/{subrredit_str}.csv'):\n",
    "                continue\n",
    "            current_data = []\n",
    "            subreddit_obj = reddit.subreddit(subrredit_str)\n",
    "            posts = list(subreddit_obj.search(query, time_filter=\"all\", limit=500, sort=sort))\n",
    "            for post in tqdm(posts, desc=f\"{query}_{sort}_{subrredit_str}\"):\n",
    "                if post.id in exisiting_data.post_id.values:\n",
    "                    continue\n",
    "                try:\n",
    "                    post_data = extract_data_from_post(post)\n",
    "                    post_data.update(dict(subreddit=subrredit_str))\n",
    "                    current_data.append(post_data)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            df = pd.DataFrame(current_data)\n",
    "            if df.shape[0] == 0:\n",
    "                continue\n",
    "            df.to_csv(f'{folder}/{subrredit_str}.csv', index=False)\n",
    "            all_data.extend(current_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### combine all the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out bad posts\n",
    "data = pd.DataFrame(all_data)\n",
    "data = data.drop_duplicates(subset=[\"post_id\"])\n",
    "data = data[~data.post_id.isin(exisiting_data.post_id)]\n",
    "df = df[(df['longest_comment_body'] != '[deleted]') &  (df['highest_comment_body'] != '[deleted]')]\n",
    "df = df[df[\"body\"].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get metadata\n",
    "df['body'] = [remove_text_after_x(text, 'edit:') for text in df['body'].values]\n",
    "df['longest_comment_body'] = [remove_text_after_x(text, 'edit:') for text in df['longest_comment_body'].values]\n",
    "df['highest_comment_body'] = [remove_text_after_x(text, 'edit:') for text in df['highest_comment_body'].values]\n",
    "df['post_body'] = df['title'] + '\\n' + df['body']\n",
    "df[\"deleted\"] = df[\"body\"].apply(lambda x: is_deleted_or_null(x))\n",
    "df = df[df[\"deleted\"] == False]\n",
    "df[\"post_num_words\"] = df[\"post_body\"].apply(lambda x: num_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def is_deleted_or_null(text):\n",
    "    if text is None:\n",
    "        return True\n",
    "    if type(text) == float:\n",
    "        return True\n",
    "    if type(text) == str:\n",
    "        return '[deleted]' in text\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"deleted\"] == False]\n",
    "df_filtered = df_filtered[(df_filtered['post_num_words'] > 50) & (df_filtered['post_num_words'] < 1200)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(r'/data/home/ilanit.sobol/dv/data/outputs/reddit/reddit_posts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
